{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using LangChain and LLMs to Analyze Data in Amazon RDS\n",
    "\n",
    "Demonstration of [LangChain SQL Chain](https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html) (`SQLDatabaseChain` and `SQLDatabaseSequentialChain`) and [SQL Database Agent](https://python.langchain.com/en/latest/modules/agents/toolkits/examples/sql_database.html) to analyze the data in an [Amazon RDS for PostgreSQL](https://aws.amazon.com/rds/postgresql/) database. Demonstration uses OpenAI's LLMs via an API.\n",
    "\n",
    "Author: Gary A. Stafford  \n",
    "Date: 2023-05-29  \n",
    "License: MIT  \n",
    "Kernal: `conda_python3`  \n",
    "References:\n",
    "- [LangChain Documentation: SQL Chain example](https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html#sql-chain-example)\n",
    "- [LangChain Blog: LLMs and SQL](https://blog.langchain.dev/llms-and-sql/)\n",
    "- [How do davinci and text-davinci-003 differ?\n",
    "](https://help.openai.com/en/articles/6643408-how-do-davinci-and-text-davinci-003-differ)\n",
    "- [How do text-davinci-002 and text-davinci-003 differ?\n",
    "](https://help.openai.com/en/articles/6779149-how-do-text-davinci-002-and-text-davinci-003-differ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Import [The Museum of Modern Art (MoMA) Collection database](https://github.com/MuseumofModernArt/collection), found on GitHub, into an [Amazon RDS for PostgreSQL](https://aws.amazon.com/rds/postgresql/) database.\n",
    "\n",
    "2. Create a new [Amazon SageMaker notebook instance](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html) for this demonstration. Make sure your RDS instance is accessible to your SageMaker Notebook environment.\n",
    "\n",
    "3. `git clone` this post's GitHub project to your Amazon SageMaker notebook instance.\n",
    "\n",
    "4. Create or update the `.env` file, used by `dotenv`, using the terminal in your SageMaker Notebook environment. A sample `env.txt` file in the project.\n",
    "\n",
    "5. Add your RDS database credentials to the file: `RDS_ENDPOINT`, `RDS_PORT`, `RDS_USERNAME`, `RDS_PASSWORD`, `RDS_DB_NAME`. See this post's GitHub project for an example.\n",
    "\n",
    "6. Create an OpenAI account and update the `.env` file to include your OpenAI API Key.\n",
    "\n",
    "__NOTE__: When using `dotenv`, credentials will be stored in plain text. The recommended and more secure method is to use [AWS Secrets Manager](https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required for ChromaDB in Amazon Jumpstart environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update -qq && apt-get install -y build-essential -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: update pip\n",
    "%pip install pip -Uq\n",
    "\n",
    "# Install the latest versions of langchain and openai\n",
    "%pip install langchain openai -Uq\n",
    "\n",
    "# Install latest versions of other required packages\n",
    "%pip install awscli boto3 python-dotenv SQLAlchemy psycopg2-binary chromadb -Uq\n",
    "%pip install pyyaml -q\n",
    "\n",
    "# Avoid issues with install\n",
    "# https://github.com/aws/amazon-sagemaker-examples/issues/1890#issuecomment-758871546\n",
    "%pip install sentence-transformers -Uq --no-cache-dir #--force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: restart kernel to update packages\n",
    "# import os\n",
    "# os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check verions of LangChain and OpenAI\n",
    "%pip list | grep \"langchain\\|openai\\|sentence-transformers\\|SQLAlchemy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment Variable\n",
    "\n",
    "Use `dotenv` to load the OpenAI and RDS environment variables. __NOTE__: credentials will be stored in plain text. The recommended, more secure method is to use [AWS Secrets Manager](https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Avoid huggingface/tokenizers parallelism error\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env vars from .env file\n",
    "%load_ext dotenv\n",
    "\n",
    "# %reload_ext dotenv\n",
    "\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLAlchemy 2.0 reference: https://docs.sqlalchemy.org/en/20/dialects/postgresql.html\n",
    "# URI format: postgresql+psycopg2://user:pwd@hostname:port/dbname\n",
    "\n",
    "RDS_DB_NAME = os.environ.get(\"RDS_DB_NAME\")\n",
    "RDS_ENDPOINT = os.environ.get(\"RDS_ENDPOINT\")\n",
    "RDS_PASSWORD = os.environ.get(\"RDS_PASSWORD\")\n",
    "RDS_PORT = os.environ.get(\"RDS_PORT\")\n",
    "RDS_USERNAME = os.environ.get(\"RDS_USERNAME\")\n",
    "RDS_URI = f\"postgresql+psycopg2://{RDS_USERNAME}:{RDS_PASSWORD}@{RDS_ENDPOINT}:{RDS_PORT}/{RDS_DB_NAME}\"\n",
    "\n",
    "# print URI\n",
    "RDS_URI_PRINT = RDS_URI.replace(\n",
    "    RDS_ENDPOINT, \"******.******.us-east-1.rds.amazonaws.com\"\n",
    ")\n",
    "RDS_URI_PRINT = RDS_URI_PRINT.replace(RDS_PASSWORD, \"******\")\n",
    "print(RDS_URI_PRINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LangChain with OpenAI's LLMs\n",
    "\n",
    "Use OpenAI's `text-davinci-003` or `gpt-3.5-turbo` LLMs. See OpenAI's [Models Overview](https://platform.openai.com/docs/models/overview) for model information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SQLDatabase, SQLDatabaseChain, OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import SQLDatabaseSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = OpenAI(model_name=\"text-davinci-003\", temperature=0, verbose=True)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LangChain's SQL Chain\n",
    "\n",
    "Next, we will use LangChain's [SQLDatabaseChain](https://python.langchain.com/en/latest/modules/chains/examples) and [SQLDatabaseSequentialChain](https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html#sqldatabasesequentialchain) for answering questions of the MoMA database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A few sample questions\n",
    "QUESTION_01 = \"How many artists are there?\"\n",
    "QUESTION_02 = \"How many artworks are there?\"\n",
    "QUESTION_03 = \"How many rows are in the artists table?\"\n",
    "QUESTION_04 = \"How many rows are in the artworks table?\"\n",
    "QUESTION_05 = \"How many artists are there whose nationality is French?\"\n",
    "QUESTION_06 = \"How many artworks were created by artists whose nationality is Spanish?\"\n",
    "QUESTION_07 = \"How many artist names start with 'M'?\"\n",
    "QUESTION_08 = \"What nationality produced the most number of artworks?\"\n",
    "QUESTION_09 = \"How many artworks are by Claude Monet?\"\n",
    "QUESTION_10 = \"What is the oldest artwork in the collection?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sqlalchemy.exc import ProgrammingError\n",
    "\n",
    "db = SQLDatabase.from_uri(RDS_URI)\n",
    "\n",
    "db_chain = SQLDatabaseSequentialChain.from_llm(\n",
    "    llm, db, verbose=True, use_query_checker=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    db_chain.run(QUESTION_05)\n",
    "except (ProgrammingError, ValueError) as exc:\n",
    "    print(f\"\\n\\n{exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Options: Custom Table Info and Query Checker\n",
    "\n",
    "According to LangChain's [documentation](https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html#custom-table-info), \"_In some cases, it can be useful to provide custom table information instead of using the automatically generated table definitions and the first sample_rows_in_table_info sample rows._\" Of course, this is impractical when dealing with a large number of tables.\n",
    "\n",
    "\"_Sometimes the Language Model generates invalid SQL with small mistakes that can be self-corrected using the same technique used by the SQL Database Agent to try and fix the SQL using the LLM. You can simply specify this option when creating the chain:\n",
    "\n",
    "_\n",
    "According to LangChain's [documentation](https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html#adding-example-rows-from-each-table), \"_Sometimes the Language Model generates invalid SQL with small mistakes that can be self-corrected using the same technique used by the SQL Database Agent to try and fix the SQL using the LLM._\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_table_info = {\n",
    "    \"artists\": \"\"\"CREATE TABLE artists (\n",
    "        artist_id integer NOT NULL,\n",
    "        name character varying(200),\n",
    "        nationality character varying(50),\n",
    "        gender character varying(25),\n",
    "        birth_year integer,\n",
    "        death_year integer,\n",
    "        CONSTRAINT artists_pk PRIMARY KEY (artist_id))\n",
    "\n",
    "/*\n",
    "3 rows from artists table:\n",
    "\"artist_id\"\t\"name\"\t\"nationality\"\t\"gender\"\t\"birth_year\"\t\"death_year\"\n",
    "12\t\"Jüri Arrak\"\t\"Estonian\"\t\"Male\"\t1936\t\n",
    "19\t\"Richard Artschwager\"\t\"American\"\t\"Male\"\t1923\t2013\n",
    "22\t\"Isidora Aschheim\"\t\"Israeli\"\t\"Female\"\t\t\n",
    "*/\"\"\",\n",
    "    \"artworks\": \"\"\"CREATE TABLE artworks (\n",
    "        artwork_id integer NOT NULL,\n",
    "        title character varying(500),\n",
    "        artist_id integer NOT NULL,\n",
    "        name character varying(500),\n",
    "        date integer,\n",
    "        medium character varying(250),\n",
    "        dimensions text,\n",
    "        acquisition_date text,\n",
    "        credit text,\n",
    "        catalogue character varying(250),\n",
    "        department character varying(250),\n",
    "        classification character varying(250),\n",
    "        object_number text,\n",
    "        diameter_cm text,\n",
    "        circumference_cm text,\n",
    "        height_cm text,\n",
    "        length_cm text,\n",
    "        width_cm text,\n",
    "        depth_cm text,\n",
    "        weight_kg text,\n",
    "        durations integer,\n",
    "        CONSTRAINT artworks_pk PRIMARY KEY (artwork_id))\n",
    "\n",
    "/*\n",
    "3 rows from artworks table:\n",
    "\"artwork_id\"\t\"title\"\t\"artist_id\"\t\"name\"\t\"date\"\t\"medium\"\t\"dimensions\"\t\"acquisition_date\"\t\"credit\"\t\"catalogue\"\t\"department\"\t\"classification\"\t\"object_number\"\t\"diameter_cm\"\t\"circumference_cm\"\t\"height_cm\"\t\"length_cm\"\t\"width_cm\"\t\"depth_cm\"\t\"weight_kg\"\t\"durations\"\n",
    "102312\t\"Watching the Game\"\t2422\t\"John Gutmann\"\t1934\t\"Gelatin silver print\"\t\"9 3/4 x 6 7/16' (24.8 x 16.4 cm)\"\t\"2006-05-11\"\t\"Purchase\"\t\"N\"\t\"Photography\"\t\"Photograph\"\t\"397.2006\"\t\t\t\"24.8\"\t\t\"16.4\"\t\t\t\n",
    "103321\t\"Untitled (page from Sump)\"\t25520\t\"Jerome Neuner\"\t1994\t\"Page with chromogenic color print and text\"\t\"12 x 9 1/2' (30.5 x 24.1 cm)\"\t\"2006-05-11\"\t\"E.T. Harmax Foundation Fund\"\t\"N\"\t\"Photography\"\t\"Photograph\"\t\"415.2006.12\"\t\t\t\"30.4801\"\t\t\"24.13\"\t\t\t\n",
    "10\t\"The Manhattan Transcripts Project, New York, New York, Episode 1: The Park\"\t7056\t\"Bernard Tschumi\"\t\t\"Gelatin silver photograph\"\t\"14 x 18' (35.6 x 45.7 cm)\"\t\"1995-01-17\"\t\"Purchase and partial gift of the architect in honor of Lily Auchincloss\"\t\"Y\"\t\"Architecture & Design\"\t\"Architecture\"\t\"3.1995.11\"\t\t\t\"35.6\"\t\t\"45.7\"\t\t\t\n",
    "*/\"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = SQLDatabase.from_uri(\n",
    "    RDS_URI,\n",
    "    include_tables=[\"artists\", \"artworks\"],\n",
    "    sample_rows_in_table_info=3,\n",
    "    custom_table_info=custom_table_info,\n",
    ")\n",
    "\n",
    "db_chain = SQLDatabaseSequentialChain.from_llm(\n",
    "    llm, db, verbose=True, use_query_checker=True, top_k=3\n",
    ")\n",
    "\n",
    "try:\n",
    "    db_chain.run(QUESTION_05)\n",
    "except (ProgrammingError, ValueError) as exc:\n",
    "    print(f\"\\n\\n{exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize Prompt and Return Intermediate Steps\n",
    "\n",
    "For this part of the demonstration, we will also use a `PromptTemplate`. LangChain's [Prompt Templates](https://python.langchain.com/en/latest/modules/prompts/prompt_templates.html). According to LangChain, \"_A prompt template refers to a reproducible way to generate a prompt. It contains a text string (“the template”), that can take in a set of parameters from the end user and generate a prompt._\"\n",
    "\n",
    "According to LangChain's [documentation](https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html#return-intermediate-steps), \"_You can also return the intermediate steps of the `SQLDatabaseChain`. This allows you to access the SQL statement that was generated, as well as the result of running that against the SQL Database._\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_DEFAULT_TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\n",
    "Use the following format:\n",
    "\n",
    "Question: \"Question here\"\n",
    "SQLQuery: \"SQL Query to run\"\n",
    "SQLResult: \"Result of the SQLQuery\"\n",
    "Answer: \"Final answer here\"\n",
    "\n",
    "Only use the following tables:\n",
    "\n",
    "{table_info}\n",
    "\n",
    "If someone asks for the art table, they really mean the artworks table.\n",
    "\n",
    "Only single quotes in the SQLQuery.\n",
    "\n",
    "Question: {input}\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"input\", \"table_info\", \"dialect\"], template=_DEFAULT_TEMPLATE\n",
    ")\n",
    "\n",
    "# Revert to db without custom_table_info\n",
    "# Could overflow context window (max prompt+completion length) of 4097\n",
    "db = SQLDatabase.from_uri(RDS_URI)\n",
    "\n",
    "db_chain = SQLDatabaseChain.from_llm(\n",
    "    llm,\n",
    "    db,\n",
    "    prompt=PROMPT,\n",
    "    verbose=True,\n",
    "    use_query_checker=True,\n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = db_chain(QUESTION_05)\n",
    "except (ProgrammingError, ValueError) as exc:\n",
    "    print(f\"\\n\\n{exc}\")\n",
    "\n",
    "result[\"intermediate_steps\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Few-shot Learning\n",
    "\n",
    "To improve the accuracy of the SQL query, LangChain allows us to use few-shot learning (aka few-shot prompting). According to [Wikipedia](https://en.wikipedia.org/wiki/In-context_learning_(natural_language_processing), \"_In natural language processing, in-context learning, few-shot learning or few-shot prompting is a prompting technique that allows a model to process examples before attempting a task. The method was popularized after the advent of GPT-3 and is considered to be an emergent property of large language models._\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import yaml\n",
    "\n",
    "chain = SQLDatabaseChain.from_llm(\n",
    "    llm, db, verbose=True, return_intermediate_steps=True, use_query_checker=True\n",
    ")\n",
    "\n",
    "\n",
    "def _parse_example(result: Dict) -> Dict:\n",
    "    sql_cmd_key = \"sql_cmd\"\n",
    "    sql_result_key = \"sql_result\"\n",
    "    table_info_key = \"table_info\"\n",
    "    input_key = \"input\"\n",
    "    final_answer_key = \"answer\"\n",
    "\n",
    "    _example = {\n",
    "        \"input\": result.get(\"query\"),\n",
    "    }\n",
    "\n",
    "    steps = result.get(\"intermediate_steps\")\n",
    "    answer_key = sql_cmd_key  # the first one\n",
    "    for step in steps:\n",
    "        # The steps are in pairs, a dict (input) followed by a string (output).\n",
    "        # Unfortunately there is no schema but you can look at the input key of the\n",
    "        # dict to see what the output is supposed to be\n",
    "        if isinstance(step, dict):\n",
    "            # Grab the table info from input dicts in the intermediate steps once\n",
    "            if table_info_key not in _example:\n",
    "                _example[table_info_key] = step.get(table_info_key)\n",
    "\n",
    "            if input_key in step:\n",
    "                if step[input_key].endswith(\"SQLQuery:\"):\n",
    "                    answer_key = sql_cmd_key  # this is the SQL generation input\n",
    "                if step[input_key].endswith(\"Answer:\"):\n",
    "                    answer_key = final_answer_key  # this is the final answer input\n",
    "            elif sql_cmd_key in step:\n",
    "                _example[sql_cmd_key] = step[sql_cmd_key]\n",
    "                answer_key = sql_result_key  # this is SQL execution input\n",
    "        elif isinstance(step, str):\n",
    "            # The preceding element should have set the answer_key\n",
    "            _example[answer_key] = step\n",
    "    return _example\n",
    "\n",
    "\n",
    "example: any\n",
    "try:\n",
    "    result = chain(QUESTION_05)\n",
    "    print(\"\\n*** Query succeeded\")\n",
    "    example = _parse_example(result)\n",
    "except Exception as exc:\n",
    "    print(\"\\n*** Query failed\")\n",
    "    result = {\"query\": QUESTION_05, \"intermediate_steps\": exc.intermediate_steps}\n",
    "    example = _parse_example(result)\n",
    "\n",
    "\n",
    "# print for now, in reality you may want to write this out to a YAML file or database for manual fix-ups offline\n",
    "yaml_example = yaml.dump(example, allow_unicode=True)\n",
    "print(\"\\n\" + yaml_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the corrected examples for few shot prompt examples\n",
    "SQL_SAMPLES = None\n",
    "\n",
    "with open(\"../few_shot_examples/sql_examples_postgresql.yaml\", \"r\") as stream:\n",
    "    SQL_SAMPLES = yaml.safe_load(stream)\n",
    "\n",
    "print(yaml.dump(SQL_SAMPLES[0], allow_unicode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.chains.sql_database.prompt import _postgres_prompt, PROMPT_SUFFIX\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts.example_selector.semantic_similarity import (\n",
    "    SemanticSimilarityExampleSelector,\n",
    ")\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"table_info\", \"input\", \"sql_cmd\", \"sql_result\", \"answer\"],\n",
    "    template=\"{table_info}\\n\\nQuestion: {input}\\nSQLQuery: {sql_cmd}\\nSQLResult: {sql_result}\\nAnswer: {answer}\",\n",
    ")\n",
    "\n",
    "examples_dict = SQL_SAMPLES\n",
    "\n",
    "local_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples_dict,\n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    local_embeddings,\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma,  # type: ignore\n",
    "    # This is the number of examples to produce and include per prompt\n",
    "    k=min(3, len(examples_dict)),\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=_postgres_prompt + \"Here are some examples:\",\n",
    "    suffix=PROMPT_SUFFIX,\n",
    "    input_variables=[\"table_info\", \"input\", \"top_k\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain.from_llm(\n",
    "    llm,\n",
    "    db,\n",
    "    prompt=few_shot_prompt,\n",
    "    use_query_checker=True,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = db_chain(QUESTION_05)\n",
    "except (ProgrammingError, ValueError) as exc:\n",
    "    print(f\"\\n\\n{exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain SQL Database Agent\n",
    "\n",
    "According to LangChain [documentation](https://python.langchain.com/en/latest/modules/agents/toolkits/examples/sql_database.html#sql-database-agent), the SQL Database Agent \"_builds off of `SQLDatabaseChain` and is designed to answer more general questions about a database, as well as recover from errors._\" __NOTE__: _it is not guaranteed that the agent won’t perform DML statements on your database given certain questions. Be careful running it on sensitive data!_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of describing a table using the agent\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "\n",
    "agent_executor = create_sql_agent(llm=llm, toolkit=toolkit, verbose=True)\n",
    "\n",
    "try:\n",
    "    agent_executor.run(\"Describe the artists table.\")\n",
    "except (ProgrammingError, ValueError) as exc:\n",
    "    print(f\"\\n\\n{exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of running queries using the agent\n",
    "try:\n",
    "    agent_executor.run(QUESTION_05)\n",
    "except (ProgrammingError, ValueError) as exc:\n",
    "    print(f\"\\n\\n{exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
