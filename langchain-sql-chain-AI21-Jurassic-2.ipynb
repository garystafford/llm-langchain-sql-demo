{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LangChain and LLM to Analyze Data in an Amazon RDS Database\n",
    "\n",
    "Demonstration of [LangChain SQL Chain](https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html) (`SQLDatabaseChain` and `SQLDatabaseSequentialChain`) to analyze the data in an [Amazon RDS for PostgreSQL](https://aws.amazon.com/rds/postgresql/) database. Demonstration uses a variety of LLMs hosted on AWS SageMaker. Kernal used: `conda_python3`.\n",
    "\n",
    "Author: Gary A. Stafford  \n",
    "Date: 2023-05-21  \n",
    "License: Apache-2.0  \n",
    "References:\n",
    "- [LangChain Documentation: SQL Chain example](https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html#sql-chain-example)\n",
    "- [LangChain Documentation: SageMaker Enpoint](https://python.langchain.com/en/latest/modules/models/llms/integrations/sagemaker.html#sagemakerendpoint)\n",
    "- [AI21 Jurassic-2 Grande](https://aws.amazon.com/marketplace/pp/prodview-5ytkctg7ux5om)\n",
    "- [LangChain Blog: LLMs and SQL](https://blog.langchain.dev/llms-and-sql/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Import the [The Museum of Modern Art (MoMA) Collection database](https://github.com/MuseumofModernArt/collection), found on GitHub, into an [Amazon RDS for PostgreSQL](https://aws.amazon.com/rds/postgresql/) database. Make sure your RDS instance is accessible to your SageMaker Notebook environment.\n",
    "\n",
    "2. `git clone` this post's GitHub project to your SageMaker Notebook environment.\n",
    "\n",
    "3. Create or update the `.env` file, used by `dotenv`, using the terminal in your SageMaker Notebook environment. Add your RDS database credentials to the file: `RDS_ENDPOINT`, `RDS_PORT`, `RDS_USERNAME`, `RDS_PASSWORD`, `RDS_DB_NAME`. See this post's GitHub project for an example. __NOTE__: credentials will be stored in plain text. The recommended, more secure method is to use [AWS Secrets Manager](https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html).\n",
    "\n",
    "4. Deploy Amazon SageMaker real-time inference endpoint(s) for the [AI21 Jurassic-2 Jumbo LLM](https://aws.amazon.com/marketplace/pp/prodview-uoiv2vgantros) and optionally, the [AI21 Jurassic-2 Grande LLM](https://aws.amazon.com/marketplace/pp/prodview-5ytkctg7ux5om).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\"HF_MODEL_ID\": \"google/flan-t5-xxl\", \"HF_TASK\": \"text2text-generation\"}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    transformers_version=\"4.26.0\",\n",
    "    pytorch_version=\"1.13.1\",\n",
    "    py_version=\"py39\",\n",
    "    env=hub,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,  # number of instances\n",
    "    instance_type=\"ml.m5.xlarge\",  # ec2 instance type\n",
    ")\n",
    "\n",
    "predictor.predict(\n",
    "    {\n",
    "        \"inputs\": \"The answer to the universe is\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: update pip\n",
    "%pip install pip -Uq\n",
    "\n",
    "# Install the latest versions of langchain and openai\n",
    "%pip install langchain openai -Uq\n",
    "\n",
    "# Install latest versions of other required packages\n",
    "%pip install ipywidgets awscli boto3 python-dotenv SQLAlchemy psycopg2 psycopg2-binary -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check verions of LangChain and OpenAI\n",
    "%pip list | grep 'langchain\\|openai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, os, json, psycopg2\n",
    "from langchain import SQLDatabase, SQLDatabaseChain\n",
    "from langchain.chains import SQLDatabaseSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid the error:\n",
    "# huggingface/tokenizers: The current process just got forked, after parallelism has already been used.\n",
    "# Disabling parallelism to avoid deadlocks...\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment Variable\n",
    "\n",
    "Use `dotenv` to load the RDS environment variable. __NOTE__: credentials will be stored in plain text. The recommended, more secure method is to use [AWS Secrets Manager](https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env vars from .env file\n",
    "%load_ext dotenv\n",
    "# %reload_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLAlchemy 2.0\n",
    "# https://docs.sqlalchemy.org/en/20/dialects/postgresql.html\n",
    "# uri format: postgresql+psycopg2://user:psw@hostname:port/dbname\n",
    "\n",
    "RDS_DB_NAME = os.environ.get(\"RDS_DB_NAME\")\n",
    "RDS_ENDPOINT = os.environ.get(\"RDS_ENDPOINT\")\n",
    "RDS_PASSWORD = os.environ.get(\"RDS_PASSWORD\")\n",
    "RDS_PORT = os.environ.get(\"RDS_PORT\")\n",
    "RDS_USERNAME = os.environ.get(\"RDS_USERNAME\")\n",
    "RDS_URI = f\"postgresql+psycopg2://{RDS_USERNAME}:{RDS_PASSWORD}@{RDS_ENDPOINT}:{RDS_PORT}/{RDS_DB_NAME}\"\n",
    "\n",
    "print(RDS_URI.replace(RDS_PASSWORD, \"********\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Real-time Inference LLM Endpoint\n",
    "\n",
    "For this demo I have created two [Amazon SageMaker real-time inference endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html), each supporting [AI21 Jurassic-2 Models](https://docs.ai21.com/docs/jurassic-2-models). The first endpoint is backed by the mid-sized [AI21 Jurassic-2 Grande](https://docs.ai21.com/docs/jurassic-2-models#jurassic-2-grande-optimal-balance-of-quality-speed-and-cost) LLM (Foundation Model) and the second endpoint is backed by a larger [AI21 Jurassic-2 Jumbo](https://docs.ai21.com/docs/jurassic-2-models#jurassic-2-jumbo-unmatched-quality) LLM. I have done this to compare the quality of the SQL statements created different size LLM. You only need to deploy a single endpoint to follow along with this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon SageMaker AWS Region\n",
    "REGION_NAME = \"us-east-1\"\n",
    "\n",
    "# AI21 Jurassic-2 Grande LLM endpoint\n",
    "# running on a ml.g5.12xlarge instance\n",
    "ENDPOINT_SML = \"j2-grande\"\n",
    "\n",
    "# AI21 Jurassic-2 Jumbo LLM endpoint\n",
    "# running on a ml.g5.12xlarge instance\n",
    "ENDPOINT_LRG = \"j2-jumbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization\n",
    "\n",
    "Perform a simple text summarization to test the model's inference endpoint. Example is from the AWS Blog post, [Zero-shot prompting for the Flan-T5 foundation model in Amazon SageMaker JumpStart](https://aws.amazon.com/blogs/machine-learning/zero-shot-prompting-for-the-flan-t5-foundation-model-in-amazon-sagemaker-jumpstart/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install \"ai21[SM]\" -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ai21\n",
    "\n",
    "DESCRIPTION = \"\"\"Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document. Use Amazon Comprehend to create new products based on understanding the structure of documents. For example, using Amazon Comprehend you can search social networking feeds for mentions of products or scan an entire document repository for key phrases.\n",
    "You can access Amazon Comprehend document analysis capabilities using the Amazon Comprehend console or using the Amazon Comprehend APIs. You can run real-time analysis for small workloads or you can start asynchronous analysis jobs for large document sets. You can use the pre-trained models that Amazon Comprehend provides, or you can train your own custom models for classification and entity recognition.\n",
    "All of the Amazon Comprehend features accept UTF-8 text documents as the input. In addition, custom classification and custom entity recognition accept image files, PDF files, and Word files as input.\n",
    "Amazon Comprehend can examine and analyze documents in a variety of languages, depending on the specific feature. For more information, see Languages supported in Amazon Comprehend. Amazon Comprehend's Dominant language capability can examine documents and determine the dominant language for a far wider selection of languages.\"\"\"\n",
    "\n",
    "prompt = f\"Write a short summary for this product description: {DESCRIPTION}\"\n",
    "\n",
    "response = ai21.Completion.execute(\n",
    "    sm_endpoint=ENDPOINT_SML,\n",
    "    prompt=prompt,\n",
    "    maxTokens=100,\n",
    "    temperature=1,\n",
    "    stopSequences=[\"##\"],\n",
    "    numResults=1,\n",
    ")\n",
    "\n",
    "\n",
    "for comp in response[\"completions\"]:\n",
    "    print(comp[\"data\"][\"text\"].strip())\n",
    "    print(\"=============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain SageMakerEndpoint\n",
    "\n",
    "Create a [LangChain SageMakerEndpoint](https://python.langchain.com/en/latest/modules/models/llms/integrations/sagemaker.html) for each model. For this demo, I have deployed two endpoints, a small and a large LLM. The below code was referenced from the AWS Blog post, [Question answering using Retrieval Augmented Generation with foundation models in Amazon SageMaker JumpStart](https://aws.amazon.com/blogs/machine-learning/question-answering-using-retrieval-augmented-generation-with-foundation-models-in-amazon-sagemaker-jumpstart/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://docs.ai21.com/docs/python-sdk-with-amazon-sagemaker\n",
    "\n",
    "from langchain.llms.sagemaker_endpoint import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase, LLMContentHandler\n",
    "from typing import Dict\n",
    "import re\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"prompt\": prompt, **model_kwargs})\n",
    "        # print(input_str.encode('utf-8'))\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        print(response_json[\"completions\"][0][\"data\"][\"text\"])\n",
    "\n",
    "        return response_json[\"completions\"][0][\"data\"][\"text\"]\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "parameters = {\"maxTokens\": 200, \"temperature\": 1, \"numResults\": 1}\n",
    "\n",
    "parameters = {\n",
    "    \"max_length\": 200,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "    \"early_stopping\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"temperature\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "# smaller model endpoint\n",
    "llm_sml = SagemakerEndpoint(\n",
    "    endpoint_name=ENDPOINT_SML,\n",
    "    region_name=REGION_NAME,\n",
    "    model_kwargs=parameters,\n",
    "    content_handler=content_handler,\n",
    ")\n",
    "\n",
    "# larger model endpoint\n",
    "llm_lrg = SagemakerEndpoint(\n",
    "    endpoint_name=ENDPOINT_LRG,\n",
    "    region_name=REGION_NAME,\n",
    "    model_kwargs=parameters,\n",
    "    content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LangChain's SQL Chain\n",
    "\n",
    "Next, we will use LangChain's [SQLDatabaseChain](https://python.langchain.com/en/latest/modules/chains/examples) and [SQLDatabaseSequentialChain](https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html#sqldatabasesequentialchain) for answering questions of the MoMA database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A few sample questions\n",
    "QUESTION_01 = \"How many artists are there?\"\n",
    "QUESTION_02 = \"How many artworks are there?\"\n",
    "QUESTION_03 = \"How many rows are in the artists table?\"\n",
    "QUESTION_04 = \"How many rows are in the artworks table?\"\n",
    "QUESTION_05 = \"How many artists are there where nationality is French?\"\n",
    "QUESTION_06 = \"How many artworks were created by artists whose nationality is Spanish?\"\n",
    "QUESTION_07 = \"How many artist names start with 'M'?\"\n",
    "QUESTION_08 = \"What nationality produced the most number of artworks?\"\n",
    "QUESTION_09 = \"How many artworks are by Claude Monet?\"\n",
    "QUESTION_10 = \"SELECT artist_id, name, nationality, gender, birth_year, death_year FROM artists LIMIT 10;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference: https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html#sqldatabasesequentialchain\n",
    "from sqlalchemy.exc import ProgrammingError\n",
    "\n",
    "db = SQLDatabase.from_uri(RDS_URI)\n",
    "\n",
    "db_chain = SQLDatabaseSequentialChain.from_llm(\n",
    "    llm_lrg, db, verbose=True, use_query_checker=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    db_chain.run(QUESTION_02)\n",
    "except ProgrammingError as exc:\n",
    "    print(f\"\\n\\n{exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Table Info\n",
    "\n",
    "According to LangChain's [documentation](https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html#custom-table-info), \"_In some cases, it can be useful to provide custom table information instead of using the automatically generated table definitions and the first sample_rows_in_table_info sample rows._\" Of course, this is impractical when dealing with a large number of tables.\n",
    "\n",
    "## Query Checker\n",
    "\n",
    "According to LangChain's [documentation](https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html#adding-example-rows-from-each-table), \"_Sometimes the Language Model generates invalid SQL with small mistakes that can be self-corrected using the same technique used by the SQL Database Agent to try and fix the SQL using the LLM._\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_table_info = {\n",
    "    \"artists\": \"\"\"CREATE TABLE artists (\n",
    "        artist_id integer NOT NULL,\n",
    "        name character varying(200)\",\n",
    "        nationality character varying(50)\",\n",
    "        gender character varying(25)\",\n",
    "        birth_year integer,\n",
    "        death_year integer,\n",
    "        CONSTRAINT artists_pk PRIMARY KEY (artist_id)\n",
    ")\n",
    "\n",
    "/*\n",
    "3 rows from artists table:\n",
    "\"artist_id\"\t\"name\"\t\"nationality\"\t\"gender\"\t\"birth_year\"\t\"death_year\"\n",
    "12\t\"Jüri Arrak\"\t\"Estonian\"\t\"Male\"\t1936\t\n",
    "19\t\"Richard Artschwager\"\t\"American\"\t\"Male\"\t1923\t2013\n",
    "22\t\"Isidora Aschheim\"\t\"Israeli\"\t\"Female\"\t\t\n",
    "*/\"\"\",\n",
    "    \"artworks\": \"\"\"CREATE TABLE artworks (\n",
    "        artwork_id integer NOT NULL,\n",
    "        title character varying(500)\",\n",
    "        artist_id integer NOT NULL,\n",
    "        name character varying(500)\",\n",
    "        date integer,\n",
    "        medium character varying(250)\",\n",
    "        dimensions text\",\n",
    "        acquisition_date text\",\n",
    "        credit text\",\n",
    "        catalogue character varying(250)\",\n",
    "        department character varying(250)\",\n",
    "        classification character varying(250)\",\n",
    "        object_number text\",\n",
    "        diameter_cm text\",\n",
    "        circumference_cm text\",\n",
    "        height_cm text\",\n",
    "        length_cm text\",\n",
    "        width_cm text\",\n",
    "        depth_cm text\",\n",
    "        weight_kg text\",\n",
    "        durations integer,\n",
    "        CONSTRAINT artworks_pk PRIMARY KEY (artwork_id)\n",
    ")\n",
    "\n",
    "/*\n",
    "3 rows from artworks table:\n",
    "\"artwork_id\"\t\"title\"\t\"artist_id\"\t\"name\"\t\"date\"\t\"medium\"\t\"dimensions\"\t\"acquisition_date\"\t\"credit\"\t\"catalogue\"\t\"department\"\t\"classification\"\t\"object_number\"\t\"diameter_cm\"\t\"circumference_cm\"\t\"height_cm\"\t\"length_cm\"\t\"width_cm\"\t\"depth_cm\"\t\"weight_kg\"\t\"durations\"\n",
    "102312\t\"Watching the Game\"\t2422\t\"John Gutmann\"\t1934\t\"Gelatin silver print\"\t\"9 3/4 x 6 7/16' (24.8 x 16.4 cm)\"\t\"2006-05-11\"\t\"Purchase\"\t\"N\"\t\"Photography\"\t\"Photograph\"\t\"397.2006\"\t\t\t\"24.8\"\t\t\"16.4\"\t\t\t\n",
    "103321\t\"Untitled (page from Sump)\"\t25520\t\"Jerome Neuner\"\t1994\t\"Page with chromogenic color print and text\"\t\"12 x 9 1/2' (30.5 x 24.1 cm)\"\t\"2006-05-11\"\t\"E.T. Harmax Foundation Fund\"\t\"N\"\t\"Photography\"\t\"Photograph\"\t\"415.2006.12\"\t\t\t\"30.4801\"\t\t\"24.13\"\t\t\t\n",
    "10\t\"The Manhattan Transcripts Project, New York, New York, Episode 1: The Park\"\t7056\t\"Bernard Tschumi\"\t\t\"Gelatin silver photograph\"\t\"14 x 18' (35.6 x 45.7 cm)\"\t\"1995-01-17\"\t\"Purchase and partial gift of the architect in honor of Lily Auchincloss\"\t\"Y\"\t\"Architecture & Design\"\t\"Architecture\"\t\"3.1995.11\"\t\t\t\"35.6\"\t\t\"45.7\"\t\t\t\n",
    "*/\"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = SQLDatabase.from_uri(\n",
    "    RDS_URI,\n",
    "    include_tables=[\"artists\", \"artworks\"],\n",
    "    sample_rows_in_table_info=3,\n",
    "    custom_table_info=custom_table_info,\n",
    ")\n",
    "\n",
    "# print(db.table_info)\n",
    "\n",
    "db_chain = SQLDatabaseSequentialChain.from_llm(\n",
    "    llm_lrg, db, verbose=True, use_query_checker=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    db_chain.run(QUESTION_06)\n",
    "except ProgrammingError as exc:\n",
    "    print(f\"\\n\\n{exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize Prompt and Return Intermediate Steps\n",
    "\n",
    "According to LangChain's [documentation](https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html#return-intermediate-steps), \"_You can also return the intermediate steps of the `SQLDatabaseChain`. This allows you to access the SQL statement that was generated, as well as the result of running that against the SQL Database._\"\n",
    "\n",
    "For this part of the demonstration, we will also use a `PromptTemplate`. LangChain's [Prompt Templates](Prompt Templates). According to LangChain, \"_A prompt template refers to a reproducible way to generate a prompt. It contains a text string (“the template”), that can take in a set of parameters from the end user and generate a prompt._\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_DEFAULT_TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\n",
    "Use the following format:\n",
    "\n",
    "Question: \"Question here\"\n",
    "SQLQuery: \"SQL Query to run\"\n",
    "SQLResult: \"Result of the SQLQuery\"\n",
    "Answer: \"Final answer here\"\n",
    "\n",
    "Only use the following tables:\n",
    "\n",
    "{table_info}\n",
    "\n",
    "If someone asks for the table art, they really mean the artworks table.\n",
    "\n",
    "Only single quotes in the SQLQuery.\n",
    "\n",
    "Question: {input}\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"input\", \"table_info\", \"dialect\"], template=_DEFAULT_TEMPLATE\n",
    ")\n",
    "db_chain = SQLDatabaseChain.from_llm(\n",
    "    llm_sml,\n",
    "    db,\n",
    "    prompt=PROMPT,\n",
    "    verbose=True,\n",
    "    use_query_checker=True,\n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = db_chain(QUESTION_02)\n",
    "    result[\"intermediate_steps\"]\n",
    "except ProgrammingError as exc:\n",
    "    print(f\"\\n\\n{exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Few-shot Learning\n",
    "\n",
    "To improve the accuracy of the SQL query, LangChain allows us to use few-shot learning (aka few-shot prompting). According to [Wikipedia](https://en.wikipedia.org/wiki/In-context_learning_(natural_language_processing), \"_In natural language processing, in-context learning, few-shot learning or few-shot prompting is a prompting technique that allows a model to process examples before attempting a task. The method was popularized after the advent of GPT-3 and is considered to be an emergent property of large language models._\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyyaml chromadb sentence_transformers -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import yaml\n",
    "\n",
    "chain = SQLDatabaseChain.from_llm(\n",
    "    llm_sml, db, verbose=True, return_intermediate_steps=True, use_query_checker=True\n",
    ")\n",
    "\n",
    "\n",
    "def _parse_example(result: Dict) -> Dict:\n",
    "    sql_cmd_key = \"sql_cmd\"\n",
    "    sql_result_key = \"sql_result\"\n",
    "    table_info_key = \"table_info\"\n",
    "    input_key = \"input\"\n",
    "    final_answer_key = \"answer\"\n",
    "\n",
    "    _example = {\n",
    "        \"input\": result.get(\"query\"),\n",
    "    }\n",
    "\n",
    "    steps = result.get(\"intermediate_steps\")\n",
    "    answer_key = sql_cmd_key  # the first one\n",
    "    for step in steps:\n",
    "        # The steps are in pairs, a dict (input) followed by a string (output).\n",
    "        # Unfortunately there is no schema but you can look at the input key of the\n",
    "        # dict to see what the output is supposed to be\n",
    "        if isinstance(step, dict):\n",
    "            # Grab the table info from input dicts in the intermediate steps once\n",
    "            if table_info_key not in _example:\n",
    "                _example[table_info_key] = step.get(table_info_key)\n",
    "\n",
    "            if input_key in step:\n",
    "                if step[input_key].endswith(\"SQLQuery:\"):\n",
    "                    answer_key = sql_cmd_key  # this is the SQL generation input\n",
    "                if step[input_key].endswith(\"Answer:\"):\n",
    "                    answer_key = final_answer_key  # this is the final answer input\n",
    "            elif sql_cmd_key in step:\n",
    "                _example[sql_cmd_key] = step[sql_cmd_key]\n",
    "                answer_key = sql_result_key  # this is SQL execution input\n",
    "        elif isinstance(step, str):\n",
    "            # The preceding element should have set the answer_key\n",
    "            _example[answer_key] = step\n",
    "    return _example\n",
    "\n",
    "\n",
    "example: any\n",
    "try:\n",
    "    result = chain(QUESTION_01)\n",
    "    print(\"*** Query succeeded\")\n",
    "    example = _parse_example(result)\n",
    "except Exception as exc:\n",
    "    print(\"*** Query failed\")\n",
    "    result = {\"query\": QUESTION_03, \"intermediate_steps\": exc.intermediate_steps}\n",
    "    example = _parse_example(result)\n",
    "\n",
    "\n",
    "# print for now, in reality you may want to write this out to a YAML file or database for manual fix-ups offline\n",
    "yaml_example = yaml.dump(example, allow_unicode=True)\n",
    "print(\"\\n\" + yaml_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the corrected examples for few shot prompt examples\n",
    "SQL_SAMPLES = None\n",
    "\n",
    "with open(\"sql_examples.yaml\", \"r\") as stream:\n",
    "    SQL_SAMPLES = yaml.safe_load(stream)\n",
    "\n",
    "# print(SQL_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.chains.sql_database.prompt import _sqlite_prompt, PROMPT_SUFFIX\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts.example_selector.semantic_similarity import (\n",
    "    SemanticSimilarityExampleSelector,\n",
    ")\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"table_info\", \"input\", \"sql_cmd\", \"sql_result\", \"answer\"],\n",
    "    template=\"{table_info}\\n\\nQuestion: {input}\\nSQLQuery: {sql_cmd}\\nSQLResult: {sql_result}\\nAnswer: {answer}\",\n",
    ")\n",
    "\n",
    "examples_dict = SQL_SAMPLES\n",
    "\n",
    "local_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples_dict,\n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    local_embeddings,\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma,  # type: ignore\n",
    "    # This is the number of examples to produce and include per prompt\n",
    "    k=min(3, len(examples_dict)),\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=_sqlite_prompt + \"Here are some examples:\",\n",
    "    suffix=PROMPT_SUFFIX,\n",
    "    input_variables=[\"table_info\", \"input\", \"top_k\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain.from_llm(\n",
    "    llm_lrg,\n",
    "    db,\n",
    "    prompt=few_shot_prompt,\n",
    "    use_query_checker=True,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = db_chain(QUESTION_06)\n",
    "except ProgrammingError as exc:\n",
    "    print(f\"\\n\\n{exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain SQL Database Agent\n",
    "\n",
    "According to LangChain [documentation](https://python.langchain.com/en/latest/modules/agents/toolkits/examples/sql_database.html#sql-database-agent), the SQL Database Agent is \"_builds off of `SQLDatabaseChain` and is designed to answer more general questions about a database, as well as recover from errors._\" __NOTE__: _it is not guaranteed that the agent won’t perform DML statements on your database given certain questions. Be careful running it on sensitive data!_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.agents import AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm_sml)\n",
    "\n",
    "agent_executor = create_sql_agent(llm=llm_sml, toolkit=toolkit, verbose=True)\n",
    "\n",
    "try:\n",
    "    agent_executor.run(\"Describe the artists table\")\n",
    "except ProgrammingError as exc:\n",
    "    print(f\"\\n\\n{exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
